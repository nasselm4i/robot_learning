env: 

  ### ANT ###
  expert_policy_file: ../../../hw1/roble/policies/experts/Ant.pkl # Relative to where you're running this script from
  expert_data: ../../../hw1/roble/expert_data/expert_data_Ant-v2.pkl  # Relative to where you're running this script from
  expert_unlabelled_data: ../../../hw1/roble/expert_data/unlabelled/unlabelled_data_Ant-v2.pkl
  exp_name: "Ant_Default"
  env_name: Ant-v2 # choices are [Ant-v2, Humanoid-v2, Walker2d-v2, HalfCheetah-v2, Hopper-v2]
  ###########

  ### HALFCHEETAH ###
  # expert_policy_file: ../../../hw1/roble/policies/experts/HalfCheetah.pkl
  # expert_data: ../../../hw1/roble/expert_data/expert_data_HalfCheetah-v2.pkl
  # expert_unlabelled_data: ../../../hw1/roble/expert_data/unlabelled/unlabelled_data_HalfCheetah-v2.pkl  
  # exp_name: "HalfCheetah_Default"
  # env_name: HalfCheetah-v2 
  ###################

  ### WALKER ###
  # expert_policy_file: ../../../hw1/roble/policies/experts/Walker2d.pkl
  # expert_data: ../../../hw1/roble/expert_data/expert_data_Walker2d-v2.pkl
  # exp_name: "Walker_Default"
  # env_name: Walker2d-v2 
  #############

  ###  HUMANOID ###
  # expert_policy_file: ../../../hw1/roble/policies/experts/Humanoid.pkl
  # expert_data: ../../../hw1/roble/expert_data/expert_data_Humanoid-v2.pkl
  # exp_name: "Humanoid_Default"
  # env_name: Humanoid-v2
  #################

  ### HOPPER ###
  # expert_policy_file: ../../../hw1/roble/policies/experts/Hopper.pkl 
  # expert_data: ../../../hw1/roble/expert_data/expert_data_Hopper-v2.pkl  
  # exp_name: "Hopper_Default"
  # env_name: Hopper-v2
  ##############

  max_episode_length: 1000
  render: false
  
alg:
  num_rollouts: 5
  train_idm: false
  do_dagger: false
  num_agent_train_steps_per_iter: 1_000 # number of gradient steps for training policy (per iter in n_iter)
  num_idm_train_steps_per_iter: 10_000
  n_iter: 1
  batch_size: 1000 # training data collected (in the env) during each iteration
  eval_batch_size: 5000 # eval data collected (in the env) for logging metrics
  train_batch_size: 300 # number of sampled data points to be used per gradient/train step
  learning_rate: 5e-3 # The learning rate for BC
  max_replay_buffer_size: 1000000 ## Size of the replay buffer
  use_gpu: False
  gpu_id: 0 # The index for the GPU (the computer you use may have more than one)
  discrete: False
  ac_dim: 0 ## This will be overridden in the code
  ob_dim: 0 ## This will be overridden in the code
  network:
    n_layers: 2
    size: 64
    layer_sizes: [64, 32]
    activation: "tanh"
    activations: ["tanh", "tanh"]
    output_activation: "identity"

logging:
  video_log_freq: 5 # How often to generate a video to log/
  scalar_log_freq: 1 # How often to log training information and run evaluation during training.
  save_params: true # Should the parameters given to the script be saved? (Always...)
  logdir: "" ## This will be overridden in the code
  random_seed: 1